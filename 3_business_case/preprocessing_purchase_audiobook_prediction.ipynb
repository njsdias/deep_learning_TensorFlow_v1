{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks business case\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "You are given data from an **Audiobook App**. Logically, it relates to the audio versions of books ONLY. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can **predict if a customer will buy again from the Audiobook company**.\n",
    "\n",
    "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertising to him/her. If we can focus our efforts SOLELY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\n",
    "\n",
    "The targets are a Boolean variable (0 or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets. So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. 6 months sounds like a reasonable time. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobook way of digesting information.\n",
    "\n",
    "The task is simple: create a machine learning algorithm, which is able to predict if a customer will buy again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "Understading the meaning of variables/features in a dataset is important for a Data Scientist develop a model.\n",
    "\n",
    "Each row represents a person\n",
    "\n",
    "- **ID**: customer ID ; We will skip it in our algorithm OK.\n",
    "- **Book length(min)_overal:** The overall book length is the sum of the lengths of all purchases.\n",
    "- **Book length(min)_average:** The average book length is basically the sum divided by the number of purchases.\n",
    "- **Price_overall:**  the overall price paid (dolars)\n",
    "- **Price_average:** the average price paid (dolars)\n",
    "- **Review:** review is a boolean: It shows if the customer left a review. This is a metric that shows engagement with the platform. We can do an assumption it is that people who leave reviews are more likely to convert again.\n",
    "- **Review 10/10:** It measures the review of a customer on a scale from 1 to 10. Logically we will only have a value for people who left a review by examining the table.\n",
    "- **Minutes Listened:** total minutes listened which is a measure of engagement next to it\n",
    "- **Completion:** It is the total minutes listened divided by the total length of books a person has purchased.\n",
    "- **Support Requests:** It shows the total number of support requests the person has opened.\n",
    "- **Last visited minus Purchase Date:**  It measures the difference between the last time a person interacted with the platform and their first purchase date. The bigger the difference the better. If a person engages regularly with the platform this difference will be bigger. Thus the customer is likely to convert again. If the value of this variable is zero we are sure the customer has never access what he has bought. Or perhaps he did it on the first day only. So it is unlikely he or she will convert again.\n",
    "- **Targets:** if he or she bought another book and if that happened we can count them as a conversion and the target will be 1. Otherwise it is zero.\n",
    "\n",
    "**Note:** The price variable is almost always a good predictor of behavior.\n",
    "\n",
    "**Review 10/10 details:**\n",
    "\n",
    "We quickly see most people leave no review.\n",
    "As in most marketplaces that's bad for our data set and bad in general.\n",
    "We can decided to leave the reviews posted to the platform and substitute all missing values with the average review.\n",
    "\n",
    "The average is 8.91. For our machine learning algorithm 8.91 one would mean the status quo.\n",
    "A review bigger than 8.9 would indicate above average feelings. While the review less than 8.91 would indicate below average feelings notice.\n",
    "\n",
    "A customer may have bought two or three books on the platform as a whole.\n",
    "An average of two out of ten indicates the person did not have a pleasant experience with audio books\n",
    "especially when the average is 8.9 one it is logical that such a customer is not likely to buy again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n",
    "\n",
    "# Select all observations that are from 2nd colum minus the last column\n",
    "# We don't have interest in ID. And the last column is the targets.\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "\n",
    "# The targets are in the last column. That's how datasets are conventionally organized.\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if Dataset is Balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>994.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>19.73</td>\n",
       "      <td>19.73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1603.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2882.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>5.96</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.42</td>\n",
       "      <td>680.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3342.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.22</td>\n",
       "      <td>475.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2      3      4    5      6     7       8    9     10  \\\n",
       "0   994.0  1620.0  1620.0  19.73  19.73  1.0  10.00  0.99  1603.8  5.0   92.0   \n",
       "1  1143.0  2160.0  2160.0   5.33   5.33  0.0   8.91  0.00     0.0  0.0    0.0   \n",
       "2  2059.0  2160.0  2160.0   5.33   5.33  0.0   8.91  0.00     0.0  0.0  388.0   \n",
       "3  2882.0  1620.0  1620.0   5.96   5.96  0.0   8.91  0.42   680.4  1.0  129.0   \n",
       "4  3342.0  2160.0  2160.0   5.33   5.33  0.0   8.91  0.22   475.2  0.0  361.0   \n",
       "\n",
       "    11  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_csv_data_df = pd.DataFrame(raw_csv_data)\n",
    "raw_csv_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of ones 0.84\n",
      "Percentage of zeros 0.16\n",
      "Unbalanced Dataset\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "perc_ones = (len(raw_csv_data_df[11]) - len(raw_csv_data_df[raw_csv_data_df[11]==1]))/len(raw_csv_data_df[11])\n",
    "\n",
    "perc_zeros = (len(raw_csv_data_df[11]) - len(raw_csv_data_df[raw_csv_data_df[11]==0]))/len(raw_csv_data_df[11])\n",
    "\n",
    "print(\"Percentage of ones {}\".format(round(perc_ones,2)))\n",
    "print(\"Percentage of zeros {}\".format(round(perc_zeros,2)))\n",
    "if LA.norm(perc_ones - perc_zeros) > 0.6:\n",
    "    print(\"Unbalanced Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many targets are 1 (meaning that the customer did convert)\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "# Set a counter for targets that are 0 (meaning that the customer did not convert)\n",
    "zero_targets_counter = 0\n",
    "\n",
    "# We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n",
    "# Declare a variable that will do that:\n",
    "indices_to_remove = []\n",
    "\n",
    "# Here we want capture the observations that have 0 value\n",
    "# in target if the total of zeros counting so far \n",
    "# is greater than thhe total of One's.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# It deletes an object along an axis\n",
    "# np..delete(array. obj_to_delete, axis)\n",
    "unscale_inputs_equal_priors = np.delete(unscaled_inputs_all,\n",
    "                                       indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, \n",
    "                                 indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Inputs\n",
    "\n",
    "Scale / Standardize inputs is very common preprocessing step in Machine Learning because it improves a lot the predictions of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscale_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the data\n",
    "\n",
    "Sometimes the data was collected in order. To make our model independent of that we shuffle inputs and outputs. \n",
    "\n",
    "Imagine the data is ordered so each batch represents approximately a different day of purchases inside the batch.\n",
    "The data is homogeneous. While between batches it is very heterogeneous, outer promotions day of the week effects and so on. This will confuse the stochastic gradient descent when we average the loss across batches.\n",
    "\n",
    "Overall we want them shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes an evenly spaced values within a given interval \n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
    "# Naturally, the numbers are integers.\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "\n",
    "# The 'test' dataset contains all remaining data.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if the datset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1780.0 3579 0.4973456272701872\n",
      "Validation:  230.0 447 0.5145413870246085\n",
      "Test:  227.0 448 0.5066964285714286\n"
     ]
    }
   ],
   "source": [
    "print('Train: ', np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print('Validation: ', np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print('Test: ', np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
