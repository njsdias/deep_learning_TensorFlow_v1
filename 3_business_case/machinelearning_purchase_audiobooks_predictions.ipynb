{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks business case\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "Here we implement our Machine Learning model to predict the purchase of audiobooks.\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "You are given data from an **Audiobook App**. Logically, it relates to the audio versions of books ONLY. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can **predict if a customer will buy again from the Audiobook company**.\n",
    "\n",
    "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertising to him/her. If we can focus our efforts SOLELY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\n",
    "\n",
    "The targets are a Boolean variable (0 or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets. So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. 6 months sounds like a reasonable time. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobook way of digesting information.\n",
    "\n",
    "The task is simple: create a machine learning algorithm, which is able to predict if a customer will buy again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class that will batch the data\n",
    "\n",
    "Whenever you want to batch the data you need to have appropriate methods. There are some batching methods integrated in TensorFlow and sklearn, but some problems may need specific coding. \n",
    "\n",
    "Here we show how these methods look like. You can use them for any machine learning framework you need (directly or after little fine-tuning).\n",
    "\n",
    "This part is more programming than Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Librabries\n",
    "import numpy as np\n",
    "\n",
    "# Create a class that will do the batching for the algorithm\n",
    "class Audiobooks_Data_Reader():\n",
    "    # dataset(string): it is the name of the dataset ; train, validation, test\n",
    "    # batch_size(integer): it is the number of portions the data will be slice. \n",
    "    #             If we don't specified an integer number the dataset is not\n",
    "    #             splice\n",
    "    def __init__(self, dataset, batch_size = None):\n",
    "        # Load the dataset\n",
    "        npz = np.load('Audiobooks_data_{0}.npz'.format(dataset))\n",
    "        \n",
    "        # store data in the variables: inputs(float); targets(integer)\n",
    "        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "        # If the batch size is None, we are either \n",
    "        # validating or testing, so we want to take the data in a single batch.\n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "        # Counts the batch number, given the size you feed it later   \n",
    "        self.curr_batch = 0\n",
    "        # Operator // is Floor Division\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "        \n",
    "    # We slice the dataset in batches and then \n",
    "    # the \"next\" function loads them one after the other\n",
    "    # This method loads the next batch\n",
    "    def __next__(self):\n",
    "        if self.curr_batch >= self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "        \n",
    "        # Define the batch slice\n",
    "        # The slice object is used to slice a given sequence \n",
    "        # slice(start,stop,step)\n",
    "        batch_slice = slice(self.curr_batch * self.batch_size, \n",
    "                            (self.curr_batch + 1) * self.batch_size)\n",
    "\n",
    "        # Slice the batch\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch = self.targets[batch_slice]\n",
    "        self.curr_batch += 1\n",
    "              \n",
    "        # One-hot encode (OHE) targets. In this example it's a bit superfluous since we have a 0/1 column \n",
    "        # However, it will be useful for any classification task with more than one target column\n",
    "        # Example: \n",
    "        # target = 0 OHE= [1,0,0] ; target = 1 OHE= [0,1,0] ; ; target = 2 OHE= [0,0,1]\n",
    "        classes_num = 2\n",
    "        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n",
    "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1  \n",
    "        \n",
    "        # The function will return the inputs batch and the one-hot encoded targets\n",
    "        return inputs_batch, targets_one_hot\n",
    "        \n",
    "    # A method needed for iterating over the batches, as we will put them in a loop\n",
    "    # This tells Python that the class we're defining is iterable, i.e. that we can use it like:\n",
    "    # for input, output in data: \n",
    "        # do things\n",
    "    # An iterator in Python is a class with a method __next__ that defines exactly how to iterate through its objects\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training loss: 0.596. Validation loss: 0.511. Validation accuracy: 78.52%\n",
      "Epoch 2. Training loss: 0.483. Validation loss: 0.428. Validation accuracy: 81.43%\n",
      "Epoch 3. Training loss: 0.425. Validation loss: 0.388. Validation accuracy: 80.76%\n",
      "Epoch 4. Training loss: 0.395. Validation loss: 0.369. Validation accuracy: 81.21%\n",
      "Epoch 5. Training loss: 0.376. Validation loss: 0.359. Validation accuracy: 80.54%\n",
      "Epoch 6. Training loss: 0.365. Validation loss: 0.353. Validation accuracy: 80.09%\n",
      "Epoch 7. Training loss: 0.357. Validation loss: 0.348. Validation accuracy: 79.87%\n",
      "Epoch 8. Training loss: 0.351. Validation loss: 0.344. Validation accuracy: 80.54%\n",
      "Epoch 9. Training loss: 0.346. Validation loss: 0.342. Validation accuracy: 80.76%\n",
      "Epoch 10. Training loss: 0.342. Validation loss: 0.339. Validation accuracy: 81.21%\n",
      "Epoch 11. Training loss: 0.339. Validation loss: 0.337. Validation accuracy: 81.43%\n",
      "Epoch 12. Training loss: 0.335. Validation loss: 0.335. Validation accuracy: 81.43%\n",
      "Epoch 13. Training loss: 0.333. Validation loss: 0.334. Validation accuracy: 81.43%\n",
      "Epoch 14. Training loss: 0.331. Validation loss: 0.333. Validation accuracy: 80.98%\n",
      "Epoch 15. Training loss: 0.329. Validation loss: 0.331. Validation accuracy: 80.98%\n",
      "Epoch 16. Training loss: 0.327. Validation loss: 0.330. Validation accuracy: 80.98%\n",
      "Epoch 17. Training loss: 0.325. Validation loss: 0.328. Validation accuracy: 80.98%\n",
      "Epoch 18. Training loss: 0.324. Validation loss: 0.328. Validation accuracy: 80.98%\n",
      "Epoch 19. Training loss: 0.322. Validation loss: 0.327. Validation accuracy: 80.98%\n",
      "Epoch 20. Training loss: 0.321. Validation loss: 0.326. Validation accuracy: 80.98%\n",
      "Epoch 21. Training loss: 0.320. Validation loss: 0.325. Validation accuracy: 80.98%\n",
      "Epoch 22. Training loss: 0.319. Validation loss: 0.324. Validation accuracy: 80.98%\n",
      "Epoch 23. Training loss: 0.318. Validation loss: 0.324. Validation accuracy: 80.98%\n",
      "Epoch 24. Training loss: 0.317. Validation loss: 0.323. Validation accuracy: 81.21%\n",
      "Epoch 25. Training loss: 0.317. Validation loss: 0.322. Validation accuracy: 81.21%\n",
      "Epoch 26. Training loss: 0.316. Validation loss: 0.321. Validation accuracy: 81.21%\n",
      "Epoch 27. Training loss: 0.315. Validation loss: 0.320. Validation accuracy: 81.43%\n",
      "Epoch 28. Training loss: 0.314. Validation loss: 0.320. Validation accuracy: 81.66%\n",
      "Epoch 29. Training loss: 0.314. Validation loss: 0.319. Validation accuracy: 81.66%\n",
      "Epoch 30. Training loss: 0.313. Validation loss: 0.319. Validation accuracy: 81.88%\n",
      "Epoch 31. Training loss: 0.313. Validation loss: 0.318. Validation accuracy: 82.33%\n",
      "Epoch 32. Training loss: 0.313. Validation loss: 0.318. Validation accuracy: 82.55%\n",
      "Epoch 33. Training loss: 0.312. Validation loss: 0.318. Validation accuracy: 82.33%\n",
      "Epoch 34. Training loss: 0.312. Validation loss: 0.317. Validation accuracy: 82.33%\n",
      "Epoch 35. Training loss: 0.311. Validation loss: 0.317. Validation accuracy: 82.55%\n",
      "Epoch 36. Training loss: 0.311. Validation loss: 0.316. Validation accuracy: 82.55%\n",
      "Epoch 37. Training loss: 0.310. Validation loss: 0.316. Validation accuracy: 82.33%\n",
      "Epoch 38. Training loss: 0.310. Validation loss: 0.315. Validation accuracy: 82.55%\n",
      "Epoch 39. Training loss: 0.310. Validation loss: 0.315. Validation accuracy: 82.55%\n",
      "Epoch 40. Training loss: 0.309. Validation loss: 0.315. Validation accuracy: 82.33%\n",
      "Epoch 41. Training loss: 0.309. Validation loss: 0.315. Validation accuracy: 82.33%\n",
      "Epoch 42. Training loss: 0.309. Validation loss: 0.315. Validation accuracy: 82.33%\n",
      "Epoch 43. Training loss: 0.308. Validation loss: 0.314. Validation accuracy: 82.33%\n",
      "Epoch 44. Training loss: 0.308. Validation loss: 0.314. Validation accuracy: 82.10%\n",
      "End of training.\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Number of features (10)\n",
    "# Book length(min)_overal , Book length(min)_average, Price_average, Review, Minutes Listened,\n",
    "# Review 10/10, Completion, Support Requests, Last visited minus, Purchase Date \n",
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50 # width\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.int32, [None, output_size])\n",
    "\n",
    "# Hidden Layer 1\n",
    "# Activation Function introduces de non-linearity to the model\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs,weights_1) + biases_1) #Activation Function: ReLu\n",
    "\n",
    "# Hidden Layer 2\n",
    "# Activation Function introduces de non-linearity to the model\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\",[hidden_layer_size])\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1,weights_2) + biases_2) #Activation Function: ReLu\n",
    "\n",
    "# Output Layer\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [output_size])\n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3\n",
    "\n",
    "# Loss Function that incorporates the activation function for the output layer\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits = outputs, labels = targets)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Optimization of the Loss Function\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "# Define the accuracy of the model\n",
    "out_equals_target = tf.equal(tf.argmax(outputs,1), tf.argmax(targets,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "# Initializer variables\n",
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "batch_size = 100\n",
    "max_epochs = 50\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "train_data = Audiobooks_Data_Reader('train', batch_size)\n",
    "validation_data = Audiobooks_Data_Reader ('validation')\n",
    "\n",
    "# Note, in validation_data we don't specify any batch_size number\n",
    "# It means that it will not be slipted in batches.\n",
    "# And for loop only occurs one time.\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        curr_epoch_loss += batch_loss\n",
    "        \n",
    "    curr_epoch_loss /= train_data.batch_count\n",
    "    \n",
    "    validation_loss = 0.\n",
    "    validation_accuracy = 0.\n",
    "    \n",
    "    for input_batch, target_batch in validation_data:\n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})   \n",
    "        \n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    prev_validation_loss = validation_loss\n",
    "    \n",
    "print('End of training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.13%\n"
     ]
    }
   ],
   "source": [
    "test_data = Audiobooks_Data_Reader('test')\n",
    "\n",
    "for input_batch, target_batch in test_data:\n",
    "        test_accuracy = sess.run([accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})   \n",
    "        \n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "print('Test Accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model\n",
    "\n",
    "Usually we can improve our model checking the below itens:\n",
    "- Improve the preprocessing\n",
    "- Fine-tune the model: increase the with and the depth of the neural network\n",
    "- Play around with the activation functions\n",
    "- Fiddle with the Batch Size: Single Batach size means Gradiante Descendent, Thousands of batches sizes means Stocastic Gradient Descendent \n",
    "- Experiment with the learning rate / optimizers: Visit the TensorFlow website to see the options that we have for optimizers\n",
    "- Try Kaggle challenges websites to adquire more knowledge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
